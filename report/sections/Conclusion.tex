\subsection{Summary of Main Results}

Benchmarking energy consumption and identifying deviations using an energy baseline model can be a scalable framework that can also be adapted to different machines in an industrial setting (as shown in \hyperlink{section.2}{Section 2} and with the analysis on the HIPE data). The specific method chosen for modeling in this thesis, namely Gaussian Processes, is also a powerful non-parametric way for modeling non-linear time series. The construction of kernels allows one to model a wide variety of processes. Furthermore, using probabilistic methods allows for the communication of uncertainty in the SPC charts and forecasted energy consumption using the posterior predictive distribution. Whereas more commonly in the literature, single point predictions are used which doesn't allow for analyzing the prediction intervals. 

Using the metering data provided by CLEMAP, an energy consumption benchmark is established by training a Gaussian Process energy baseline model with a specially designed kernel composition on a reporting period that represents the ``best operating conditions" for a piece of equipment and where the residuals fluctuate around a mean of zero. From there, using the posterior predictive distribution and SPC methodology, the EE is monitored and analyzed to identify when a machine is deviating away from the expected behavior. Thus, not only is a probabilistic energy baseline model useful in benchmarking energy consumption, but it also provides value to the equipment operator in identifying a significant deviation in EE. As a proof of concept, an example was shown using data from the paper disposal machine. Using the methods proposed, the baseline model and SPC charts identified a large decrease in energy consumption. 

\subsection{Discussion with CLEMAP's Client}

The proof of concept was presented to the production leader and technician. They had stated they were not too sure what could have caused such a significant decrease, but it could be related to the Manroland printer (R707LV), Heidelberg Speedmaster (XL106) printer, or the steel folding machine. Furthermore, when presented the time series plots, they were surprised that the machine was cycling over night, when in fact the machine should not be consuming energy.

Additionally, the results of the energy baseline models were achieved with only time as the input. Referring to \hyperlink{section.2}{Section 2}, additional data is often used to improve the predictive accuracy of the model and to provide additional insights other variables that affect EE. Data is typically compiled through two forms: (1) additional sensors, and (2) non-sensor based. In the discussion with CLEMAP's client, the production leader schedules the next week's production on the Thursday before—this represents the amount of goods the company expects to produce. The production leader is interested in using this data—combined with the energy data—to produce productivity metrics. Likewise, this production data could be used as an input, in addition to the time input, in the GP to improve the quality of the energy baseline model.

\subsection{Reflection on Research Questions}

Here, a reflection on the research questions outlined in \hyperlink{subsection.1.3}{Section 1.3} is presented. The reflection next to the corresponding number indicates which research question is being reviewed upon.

\begin{enumerate}
    \item Using only the measurement data provided by CLEMAP, a benchmark of the energy consumption is defined and modeled using an energy baseline model which represents the energy characterization of the ``starting situation" before any new production process, equipment component, or energy intervention measure is implemented. As a manufacturing environment is heterogeneous, an energy consumption benchmark should be updated \textit{after} a new production process, energy intervention measure, etc. is implemented. Updating the benchmark after allows one to measure the counterfactual, i.e., what would have happened. After this counterfactual is measured, then the benchmark should be updated. Furthermore, several benchmarks may need to be developed for a piece of equipment \textit{if} that equipment is capable of dual outputs. For example, in the printing industry, it is common for a manufacturer to have a printer that is capable of printing magazines, books, newspapers, etc. Therefore, if the manufacturer schedules these jobs in cohorts, then a benchmark should be estimated for each cohort.
    
    \item As discussed in \hyperlink{subsection.7.2}{Section 7.2} and reviewed in \hyperlink{section.2}{Section 2}, additional data such as production output, harmonic distortions or meteorological is often incorporated into the model as a covariate. The additional data may not only provide for more precise forecasts in energy consumption, but also allows for a root cause analysis for when a piece of equipment experiences a decrease in EE. That is, when equipment deviates from the expected behavior, the covariates can also be analyzed to determine if there is a correlation with the deviation in EE. Furthermore, production data can allow one to calculate KPIs. The KPIs, in turn, act as a goal for the production staff and incentive the team to meet specific performance metrics. 
    
    \item For proof of feasibility, it was determined that a batch setting was the most cost effective way of developing the models. In a batch setting, the entire data set $D$ is available before training starts. Therefore, this allowed CLEMAP to give us a ``data dump" which doesn't incur additional costs for the company in the form of increased technical support and compute. However, it is possible with CLEMAP's \ac{API} to develop the model in an online setting, i.e., the data arrives sequentially in an unbounded stream. Though, this type of modeling is more demanding in regard to compute power, API requests, and technical skill sets—all resulting in additional costs for CLEMAP
    
    \item The open source software, Docker, is used to implement a ``container" for the GP model which would then be utilized for deployment on CLEMAP's infrastructure. This container represents the \textit{inference} phase, and allows CLEMAP to incorporate and or develop this container into their production environment as needed.  
    
    \item In ensuring the energy baseline model does not overfit and is capable of generalizing to new unseen data, the simple ``holdout" method is used. That is, the data is split into two sets, namely the training and validation set. In the example with the paper disposal machine, training was done with data from Monday to Thursday (four days) and validation was performed on Friday (one day). Furthermore, since a gradient based optimization algorithm was used, early stopping is used to stop the optimization process. By analyzing the training and validation loss together, the optimization can be stopped when a divergence occurs between the training and validation loss. This stopping prevents the model from learning too much information about the training set \cite{pml1Book}. 
    
    \item In the preliminary study, it was outlined that there would be a larger focus on the ``DevOps" and model deployment of the energy baseline model. However, the direction in the main thesis focused on more of a ``condition and monitoring" approach to EEE and PDD. Therefore, no specific conclusion was reached regarding the cost factors associated with the model deployment as a model was never deployed in production on CLEMAP's infrastructure. However, as a compromise, a Docker container was developed to prepare the model for deployment on CLEMAP's infrastructure. This technique of containerizing models using Docker has become the standard for scaling and monitoring machine learning deployments. 
    
    \item Statistical process control methodologies in the form of charts and statistical measures (instantaneous change and CuSum) are used to perform performance deviation detection. Thus, PDD complements the baseline model in research question one. Furthermore, using the posterior predictive distribution of the Gaussian Process model, it is proposed to use the $95\%$ prediction interval to assess the range of plausible values instead of restricting oneself to only the standard deviation of the point estimates.  
    
    \item The SPC charts and PDD registry are shared with the production manager. The charts allow the manager to analyze instantaneous deviations in EE and deviations over time. Building off of the charts, a historical deviation registry (database) is developed to allow for further analysis on the data regarding performance deviations. For example, as more data is compiled in the registry, the maintenance staff can then analyze machine EE by observing sequences of increased energy consumption at certain time periods.
    
\end{enumerate}

\subsection{Next Steps}

Building off of the work completed in this thesis, the ``next steps" to be conducted include:

\begin{enumerate}

    \item Deploy the energy baseline model in CLEMAP's development environment using the Docker container developed in \hyperlink{subsection.6.2}{Section 6.2}. This deployment phase allows one to test the generalizability of the model on new, unseen data and to perform further model refinements. 

    \item Incorporating the expected amount of produced goods provided by the production leader on prior Thursdays as a covariate in the energy baseline model. This could enhance the predictive accuracy of the baseline model to produce more reliable forecasts and ability in performing PDD.
    
    \item Defining productivity metrics using the baseline model from step two above could be calculated. For example, the production leader schedules the next week's production on the Thursday before—this represents the amount of goods the company expects to produce. Then, using the energy baseline model, the day ahead energy consumption could be forecasted for each day. Using the production leader's forecast and the energy baseline model's energy consumption forecast, productivity metrics such as energy consumed per unit of produced good could be calculated.
    
    \item Building off of step three and following the same framework in the thesis, the difference between the forecasted productivity metrics and the actual productivity metric could be monitored to identify at which process or machine the productivity metric is deviating from the forecast.
\end{enumerate}