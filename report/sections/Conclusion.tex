\subsection{Summary of Main Results}

Using the metering data provided by CLEMAP, an energy consumption benchmark is established using a reporting period that represents the ``best operating conditions" for a piece of equipment and where the residuals fluctuate around a mean of zero. From there, using the posterior predictive distribution and SPC methodology, the EE is monitored and analyzed to identify when a machine is deviating away from the expected behavior. Thus, not only is a probabilistic energy baseline model useful in benchmarking energy consumption, but it also provides value to the client in identifying a significant deviation in EE. As a proof of concept, an example was shown using the paper disposal machine. Using the methods proposed, the baseline model and SPC charts identified a large decrease in energy consumption. When these charts were presented to the production leader and technician of CLEMAP's client, they had stated they were not too sure what could have caused such a significant decrease, but it could be related to the R707LV, XL106, or steel folding machine. Furthermore, when presented the time series plots, they were surprised that the machine was cycling over night, when in fact the machine should not be consuming energy.

\subsection{Comparisons and Differences to Current Research and Work in Industry}

Methodologies currently in use in the industry were utilized and built on top of. Here, the comparisons and differences are outlined. Referring to \hyperlink{section.2}{Section 2}, additional data is often used to improve the quality of the model and to provide additional insights into the quantification of EE and PDD. Data is typically compiled through two forms: (1) additional sensors, and (2) non-sensor based. Per the meeting with CLEMAP's client, they have data on the week ahead production schedule and are interested in using this data—combined with the energy data—to produce productivity metrics. Likewise, this production data could be used as an input, in addition to the time input, in the GP to improve the quality of the energy baseline model. In doing so, the forecasted energy consumption would also be correlated with the expected produced goods. In \cite{HIPE} \cite{boiler} \cite{gas-turbine-faults}, data related to harmonic distortions was also collected and used to quantify its relationship with energy consumption. A power system’s ability to perform at optimal levels is compromised when harmonic distortion enters the system. It creates inefficiencies in equipment operations due to the increased need for power consumption \cite{noauthor_harmonic_2021}. Therefore, harmonic distortion could also be used as a covariate to analyze equipment energy efficiency by identifying when the characteristics of the voltage waveform deviate from a sinusoidal wave, i.e., there is harmonic distortion. The probabilistic methods used in this thesis allows for the communication of uncertainty in the SPC charts and forecasted energy consumption using the posterior predictive distribution. Whereas more commonly in the literature, single point predictions are used which doesn't allow for a range of plausible values. 

\subsection{Applicability of Methods and Research Within Industry 4.0}

Benchmarking energy consumption and identifying deviations using an energy baseline model can be a scalable framework that can also be adapted to different machines in an industrial setting (as shown in \hyperlink{section.2}{Section 2} and with the analysis on the HIPE data). The specific method chosen for modeling in this thesis, namely Gaussian Processes, is also a powerful non-parametric way for modeling non-linear time series. The construction of kernels allows one to model a wide variety of processes. However, GPs have difficulty in extrapolating when the underlying physical process of a machine displays less evidence of time series components \hyperlink{subsection.3.2}{Section 3.2}. For example, the R707LV printer in Figure \ref{fig:fig19} of the appendix displays less evidence of cyclical patterns. Likewise, the underlying function has ``kinks" which is very difficult to model as most kernels generate smooth functions. Therefore, when the time series displays abrupt changes, i.e., kinks or discontinuities in the underlying function, GPs with the standard kernels presented in this thesis perform poorly in extrapolation. 

\subsection{Reflection on Research Questions}

Here, a reflection on the research questions outlined in \hyperlink{subsection.1.3}{Section 1.3} is presented. The reflection next to the corresponding number indicates which research question is being reviewed upon.

\begin{enumerate}
    \item Using only the measurement data provided by CLEMAP, a benchmark of the energy consumption is defined and modeled using an energy baseline model which represents the energy characterization of the ``starting situation" before any new production process, equipment component, or energy intervention measure is implemented. As a manufacturing environment is heterogeneous, an energy consumption benchmark should be updated \textit{after} a new production process, energy intervention measure, etc. is implemented. Updating the benchmark after allows one to measure the counterfactual, i.e., what would have happened. After this counterfactual is measured, then the benchmark should be updated. Furthermore, several benchmarks may need to be developed for a piece of equipment \textit{if} that equipment is capable of dual outputs. For example, in the printing industry, it is common for a manufacturer to have a printer that is capable of printing magazines, books, newspapers, etc. Therefore, if the manufacturer schedules these jobs in cohorts, then a benchmark should be estimated for each cohort.
    
    \item As discussed in \hyperlink{subsection.7.2}{Section 7.2} and reviewed in \hyperlink{section.2}{Section 2}, additional data is often incorporated into the model as a covariate. The additional data may not only provide for more precise forecasts in energy consumption, but also allows for a root cause analysis for when a piece of equipment experiences a decrease in EE. That is, when equipment deviates from the expected behavior, the covariates can also be analyzed to determine if there is a correlation with the deviation in EE. Furthermore, additional data such as the amount of produced goods, can allow one to calculate KPIs. The KPIs, in turn, act as a goal for the production staff to meet specific performance metrics. 
    
    \item For proof of feasibility, it was determined that a batch setting was the most cost effective way of developing the models. In a batch setting, the entire data set $D$ is available before training starts. Therefore, this allowed CLEMAP to give us a ``data dump" which doesn't incur additional costs for the company in the form of increased technical support and compute. However, it is possible with CLEMAP's \ac{API} to develop the model in an online setting, i.e., the data arrives sequentially in an unbounded stream. Though, this type of modeling is more demanding in regard to compute power, API requests, and technical skill sets—all resulting in additional costs for CLEMAP
    
    \item The open source software, Docker, is used to develop a ``container" of the GP model which would then be utilized for deployment on CLEMAP's infrastructure. This container represents the \textit{inference} phase, and allows CLEMAP to incorporate and or develop this container into their production environment as needed. Docker is an open source technology with four pricing tiers; personal, pro, team, and business at $\$0$, $\$5$, $\$7$, and $\$21$, respectively. Therefore, depending on CLEMAP's demand, Docker may be free to use. 
    
    \item In ensuring the energy baseline model does not overfit and is capable of generalizing to new unseen data, the simple ``holdout" method is used. That is, the data is split into two sets, namely the training and validation set. In the example with the paper disposal machine, training was Monday - Thursday (four days) and the validation was Friday (one day). Furthermore, since a gradient based optimization algorithm was used, early stopping is used to stop the optimization process. By analyzing the training and validation loss together, the optimization can be stopped when a divergence occurs between the training and validation loss. This stopping prevents the model from learning too much information about the training set \cite{pml1Book}. 
    
    \item In the preliminary study, it was outlined that there would be a larger focus on the ``DevOps" and model deployment of the energy baseline model. However, the direction in the main thesis focused on more of a ``condition and monitoring" approach to EEE and PDD. Therefore, no specific conclusion was reached regarding the cost factors associated with the model deployment as a model was never deployed in production on CLEMAP's infrastructure. However, as a compromise, a Docker container was developed to prepare the model for deployment on CLEMAP's infrastructure. This technique of containerizing models using Docker has become the standard for scaling and monitoring machine learning deployments. 
    
    \item Statistical process control methodologies in the form of charts and statistical measures (instantaneous change and CuSum) are used to perform performance deviation detection. Thus, PDD complements the baseline model in research question one. Furthermore, using the posterior predictive distribution of the Gaussian Process model, it is proposed to use the $95\%$ prediction interval to assess the range of plausible values instead of restricting oneself to only the standard deviation of the point estimates (mean prediction value).  
    
    \item The SPC charts and PDD registry is shared with the production manager. The charts allow the manager to analyze instantaneous deviations in EE and deviations over time. Building off of the charts, a historical deviation registry (database) is developed to allow for further analysis on the data regarding performance deviations. For example, as more data is compiled in the registry, the maintenance staff can then analyze machine EE by observing sequences of increased energy consumption at certain time periods.
    
\end{enumerate}

\subsection{Next Steps}

Next steps include leveraging the insights (incorporating production data) from a meeting with CLEMAP's client to provide additional value to monitoring EE and performing PDD. Likewise, defining productivity metrics using the baseline model could be calculated. For example, using the expected amount of produced goods for the next production week, an energy consumption forecast could then be computed using the amount of produced goods as a covariate. From there, productivity metrics could be forecasted such as energy consumed per unit of produced good (output). Then, following the same framework in the thesis, the difference between the forecasted and actual value could be monitored to identify at which process or machine the productivity metric is deviating from the forecast. 